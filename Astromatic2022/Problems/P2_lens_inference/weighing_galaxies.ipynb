{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "weighing_galaxies.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN0nbf3M/TBmAutZ4AbZrM8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ParsecInstitute/Astromatic2022/blob/hackproblemsP2/Problems/P2_lens_inference/weighing_galaxies.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJmnllDR4o7E",
        "outputId": "f27c256a-1984-422d-aec6-7cad9b30dec6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import torch\n",
        "from torch import nn\n",
        "from torchvision import transforms as T\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import h5py\n",
        "import os\n",
        "from glob import glob\n",
        "from tqdm import tqdm\n",
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "papers:\n",
        "https://arxiv.org/pdf/1208.5229.pdf -> Direct measurement of baryonic fraction\n",
        "\n",
        "The study of galaxy formation and evolution is an hot topic in current research.\n",
        "\n",
        "Gravitational lensing is a direct way to measure gravitational mass within the Einstein radius. A large population of these object would help constrain the relation between the baryonic mass fraction and the halo mass of these galaxies, which in turn would help constrain the models for galaxy formation. -> How efficient are galaxies at forming stars, etc."
      ],
      "metadata": {
        "id": "kEQfKa3H5T49"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "parameters_of_interest = [\"theta_E\"]\n",
        "\n",
        "class ProblemDataset(Dataset):\n",
        "    def __init__(\n",
        "            self,\n",
        "            dataset_dir,\n",
        "            device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n",
        "            parameters_of_interest=parameters_of_interest,\n",
        "            ):\n",
        "        super(ProblemDataset, self).__init__()\n",
        "        self.dataset_dir = dataset_dir\n",
        "        self.device = device\n",
        "        self.parameters_of_interes = parameters_of_interest\n",
        "        self.files = glob(os.path.join(dataset_dir, \"*.h5\"))\n",
        "        with h5py.File(self.files[0], \"r\") as hf:\n",
        "            # recover some information about the size of a shard\n",
        "            self.shard_size = 100\n",
        "            self.size = self.shard_size * len(self.files)\n",
        "      \n",
        "    def preprocessing(x):\n",
        "        \"\"\"\n",
        "        Define preprocessing of a single image here (shape = [channels, pix, pix])\n",
        "        \"\"\"\n",
        "        return x\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.size\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        f = index // self.shard_size\n",
        "        _index = index % self.shard_size\n",
        "        with h5py.open(self.files[f], \"r\") as hf:\n",
        "            X = hf[\"base\"][\"lenses\"][_index] # read images from disc\n",
        "            # Y = np.stack[[hf[\"base\"][][p][_index] for p in self.parameters_of_interest]] # figure out how to retrieve this\n",
        "        X = torch.tensor(X, device=self.device) # put this data into the device\n",
        "        X = self.preprocessing(X) # apply user defined transformation\n",
        "        Y = torch.tensor(Y, device=self.device) # put labels into the device\n",
        "        return X, Y\n",
        "\n"
      ],
      "metadata": {
        "id": "jc2gQ9H95v48"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Infer the mass of a lensing galaxy given a lensed image"
      ],
      "metadata": {
        "id": "_dLgQl2Aursz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define your model here\n",
        "class Model(torch.nn.Module):\n",
        "      def __init__(self, hyperparameters):\n",
        "          pass\n",
        "      \n",
        "      def forward(x):\n",
        "          pass"
      ],
      "metadata": {
        "id": "icXjzyCuU9Xl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 10\n",
        "BATCH_SIZE = 32\n",
        "LOGDIR = \"logs/\"\n",
        "LEARNING_RATE = 1e-3\n",
        "DATADIR = \"/content/drive/...\"\n",
        "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "dataset = ProblemDataset(DATADIR, device=DEVICE)\n",
        "dataset = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
        "model = Model(**hyperparameters).to(DEVICE)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "scheduler = None #define a schedule here for fine tuning\n",
        "data_augmentation = T.Compose([])\n",
        "def loss_fn(y_pred, y_true): # define your loss function here\n",
        "    return 0\n",
        "\n",
        "writer = SummaryWriter(LOGDIR)\n",
        "# ====== Training loop =========================================================\n",
        "step = 0\n",
        "for epoch in (pbar := tqdm(range(EPOCHS))):\n",
        "    cost = 0\n",
        "    for batch, (x, y) in enumerate(dataset):\n",
        "        optimizer.zero_grad()\n",
        "        # make a prediction with the model\n",
        "        y_pred = model(x)\n",
        "        # compute the loss function\n",
        "        loss = loss_fn(y_pred, y)\n",
        "        # backpropagation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # update the learning rate\n",
        "        # scheduler.step()\n",
        "# ========== Summary and logs ==================================================\n",
        "        cost += loss\n",
        "        step += 1\n",
        "\n",
        "    cost /= len(dataset)\n",
        "    writer.add_scalar(\"MSE\", cost, step)\n",
        "    writer.add_scalar(\"Learning Rate\", optimizer.param_groups[0]['lr'], step)\n",
        "    if step % 500 == 0:\n",
        "        # writer\n",
        "        print(f\"epoch {epoch} | cost {cost:.3e} \"\n",
        "        f\"| learning rate {optimizer.param_groups[0]['lr']:.2e}\")\n",
        "    writer.flush()\n",
        "    if torch.isnan(cost):\n",
        "        print(\"Training broke the Universe\")\n",
        "        break\n"
      ],
      "metadata": {
        "id": "9ehN3Wftu2l9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deblend the lens light to extract the background image"
      ],
      "metadata": {
        "id": "QbCsCLdaXd6f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "parameters_of_interest = [\"lens_light_params\"]\n",
        "\n",
        "class ProblemDataset(Dataset):\n",
        "    def __init__(\n",
        "            self,\n",
        "            dataset_dir,\n",
        "            device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n",
        "            parameters_of_interest=parameters_of_interest,\n",
        "            ):\n",
        "        super(ProblemDataset, self).__init__()\n",
        "        self.dataset_dir = dataset_dir\n",
        "        self.device = device\n",
        "        self.parameters_of_interes = parameters_of_interest\n",
        "        self.files = glob(os.path.join(dataset_dir, \"*.h5\"))\n",
        "        with h5py.File(self.files[0], \"r\") as hf:\n",
        "            # recover some information about the size of a shard\n",
        "            self.shard_size = 100\n",
        "            self.size = self.shard_size * len(self.files)\n",
        "      \n",
        "    def preprocessing(x):\n",
        "        \"\"\"\n",
        "        Define preprocessing of a single image here (shape = [channels, pix, pix])\n",
        "        \"\"\"\n",
        "        return x\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.size\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        f = index // self.shard_size\n",
        "        _index = index % self.shard_size\n",
        "        with h5py.open(self.files[f], \"r\") as hf:\n",
        "            X = hf[\"base\"][\"lenses\"][_index] # read images from disc\n",
        "            # Y = np.stack[[hf[\"base\"][][p][_index] for p in self.parameters_of_interest]] # figure out how to retrieve this\n",
        "        X = torch.tensor(X, device=self.device) # put this data into the device\n",
        "        X = self.preprocessing(X) # apply user defined transformation\n",
        "        Y = torch.tensor(Y, device=self.device) # put labels into the device\n",
        "        return X, Y\n",
        "\n"
      ],
      "metadata": {
        "id": "4iGFcrdZXjY2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}